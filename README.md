# üöï Projeto Q-Learning no Taxi-V3

Este reposit√≥rio cont√©m uma implementa√ß√£o do algoritmo **Q-Learning** para treinar um agente a resolver o ambiente **Taxi-v3** do Gymnasium (anteriormente conhecido como OpenAI Gym).

O objetivo do agente √© aprender a navegar em uma grade 5x5, buscar um passageiro em um dos quatro locais de embarque e deix√°-lo em um dos quatro locais de desembarque designados no menor n√∫mero de passos poss√≠vel.

---

## üß† O que √© Q-Learning?

**Q-Learning** √© um algoritmo de Aprendizado por Refor√ßo *Off-Policy* que permite a um agente aprender a melhor sequ√™ncia de a√ß√µes a tomar em um ambiente, maximizando uma recompensa total esperada.

O aprendizado √© armazenado em uma **Matriz Q** (Q-Table), onde cada c√©lula $(s, a)$ armazena o valor de tomar uma **a√ß√£o** ($a$) em um determinado **estado** ($s$).

### F√≥rmula Central

O cora√ß√£o do algoritmo √© a Regra de Atualiza√ß√£o da Matriz Q. No c√≥digo, a atualiza√ß√£o √© feita utilizando a seguinte f√≥rmula:

$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[r + \gamma \max_{a'} Q(s', a') - Q(s, a)\right]$$

* $Q(s, a)$: Valor Q atual para o par estado-a√ß√£o.
* $\alpha$ (Taxa de Aprendizado): Controla o quanto as novas informa√ß√µes substituem as antigas.
* $r$: Recompensa imediata.
* $\gamma$ (Fator de Desconto): Determina a import√¢ncia das recompensas futuras.
* $\max_{a'} Q(s', a')$: O maior valor Q poss√≠vel no pr√≥ximo estado ($s'$).

---

## ‚öôÔ∏è Detalhes da Implementa√ß√£o

O script `taxi_q.py` implementa as seguintes funcionalidades essenciais de um agente Q-Learning:

| Funcionalidade | Vari√°vel | Descri√ß√£o |
| :--- | :--- | :--- |
| **Matriz Q** | `q` | Tabela $500 \times 6$ para armazenar o valor de cada estado-a√ß√£o. |
| **Estrat√©gia $\epsilon$-greedy** | `epsilon` | Define a probabilidade de **explorar** (a√ß√£o aleat√≥ria) vs. **explorar** (melhor a√ß√£o conhecida). |
| **Taxa de Aprendizado** | `learning_rate_a` ($\alpha$) | Inicialmente `0.9`. Define a rapidez com que o agente aceita novas informa√ß√µes. |
| **Fator de Desconto** | `discount_factor_g` ($\gamma$) | Configurado como `0.9`, valorizando recompensas futuras. |
| **Persist√™ncia** | `pickle` | A matriz Q treinada √© salva em `taxi.pkl` para reutiliza√ß√£o e teste. |

---

## üöÄ Como Executar o Projeto

### Pr√©-requisitos

Certifique-se de ter o **Python 3** instalado.

### 1. Instala√ß√£o das Depend√™ncias

O projeto requer os pacotes `gymnasium`, `numpy` e `matplotlib`.

```bash
pip install gymnasium[classic-control] numpy matplotlib
```
---

# üìãTreinamento e Teste
O script √© configurado para realizar o treinamento e o teste automaticamente.

* Roda 1000 epis√≥dios de treinamento (run(1000)).

* Salva a matriz Q treinada em taxi.pkl.

* Roda 10 epis√≥dios de teste (run(10, is_training=False, render=True)) usando a matriz salva e exibe a visualiza√ß√£o do t√°xi em a√ß√£o.

## Para rodar:

```bash
python taxi_q.py
```
---
# üìä Resultados e Progresso
Ap√≥s o treinamento, o script gera o arquivo taxi.png, que ilustra o progresso do aprendizado. O gr√°fico exibe a soma de recompensas acumuladas em uma janela m√≥vel dos √∫ltimos 100 epis√≥dios.

Uma curva ascendente no gr√°fico indica que o agente est√° aprendendo a resolver o ambiente de forma mais eficiente, acumulando mais recompensas de sucesso (+20 por entrega) e minimizando penalidades (-1 por movimento e -10 por a√ß√µes inv√°lidas).
